{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "284dbe13-8ee9-459b-9608-f8ffb322befb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315974a4-008a-4986-bd2c-3b72ca202561",
   "metadata": {},
   "source": [
    "Hi everyone! In our last lecture, we built ***YouBandit***, an agent that learned the single best thumbnail to show on YouTube. \n",
    "\n",
    "However, this approach has a glaring weakness: it's a **one-size-fits-all** model. It assumes that the best thumbnail is the same for your little brother that is a gamer as it is for our grandmother. We are essentially assuming that every person likes the same thing. There is no personalisation.\n",
    "\n",
    "What if the best thumbnail to choose actually depends on **who is watching**? With *YouBandit*, we have no way of knowing, or storing such information, so we will never know anything personal about the user, and we can never use such information to pick the best thumbnail specifically tailored for the person watching. \n",
    "\n",
    "Today, we therefore introduce the concept of  <span style=\"color:blue\">state</span>. The best action now depends on the situation.\n",
    "\n",
    "So, we've introduced a new word, but what does it actually mean? It is actually one of the most important \n",
    "ideas in Reinforcement Learning, so it can be worth it to spend a couple of minutes to build some intuition.\n",
    "\n",
    "> <span style=\"color:blue\">state</span> is the information used to determine what happens next\n",
    "\n",
    "You can think of state as like a *summary of all the history* we have seen so far.\n",
    "\n",
    "Our new problem: We're personalising a YouTube channel's homepage.\n",
    "\n",
    "- **State**: Is the user a new viewer or a subscriber?\n",
    "- **Actions**: Should we show them a clickbaity viral thing, an AI tutorial, or a boring politics debate?\n",
    "\n",
    "Our <span style=\"color:gold\">goal</span> now is to show the right video to the right user (known as a \"Contextual Bandit\"). Solving it will force us to evolve our agent's brain. Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f453a525-611c-4411-9cbe-2198e17ece0c",
   "metadata": {},
   "source": [
    "##### Environment: the YouTube home page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f62d8d6-cb2d-453e-9409-deb1dfe32da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HomepageEnv:\n",
    "    def __init__(self):\n",
    "        self.contexts = [\"New viewer\", \"Subscriber\"]\n",
    "        self.actions = [\"🔥 Viral thing\", \"🤖 AI tutorial\", \"💤 Boring politics debate\"]\n",
    "        self.num_contexts = len(self.contexts)\n",
    "        self.num_actions = len(self.actions)\n",
    "\n",
    "    def get_reward(self, context_idx, action_idx):\n",
    "        # New viewers (context 0) are attracted by viral videos\n",
    "        if context_idx == 0: # the index for a new viewer\n",
    "            true_probs = [0.6, 0.1, 0.05] # gives the size of a spinning wheel, here \"viral thing\" has 60% of the space \n",
    "            return 1 if np.random.rand() < true_probs[action_idx] else 0 # like spinning a wheel that can land anywhere from 0.00 to 0.99\n",
    "        # Subscribers (context 1) want deeper content\n",
    "        elif context_idx == 1:\n",
    "            true_probs = [0.1, 0.5, 0.7] # The subscribers are thoughtfull and use YouTube a bit differently, they want boring debates\n",
    "            return 1 if np.random.rand() < true_probs[action_idx] else 0\n",
    "\n",
    "# Initialize environment\n",
    "env = HomepageEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "025a2015-63bd-4104-be62-d52633dbc350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['🔥 Viral thing', '🤖 AI tutorial', '💤 Boring politics debate']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9734393f-96b9-4938-87d6-4b2841e7f491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['New viewer', 'Subscriber']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d348788b-e09d-4aa9-9c72-d45b464fdf87",
   "metadata": {},
   "source": [
    "So now we have our new world created, which have the concept of a state now. The *hidden secrets* are also different depending on which user who is watching. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8d5ebf8-9d92-4110-8c81-d57d3c429b4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will now store our beliefs in a 2D table: (number of states x number of actions). here: 2x3 table\n",
    "probs = np.zeros((env.num_contexts, env.num_actions))\n",
    "probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a79f5ab-46e3-48b7-8fd6-632aab774b0f",
   "metadata": {},
   "source": [
    "\n",
    "#### YouBandit: the old way without context to establish a baseline\n",
    "\n",
    "Let's first prove that the old way is broken. We'll take our *YouBandit* from the last lecture, and see how it performs. This agent is *context-blind*. It doesn't know if a user is new or a subscriber; it just sees an action and a reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fcd6a3f0-7cab-4ef5-a072-99e70a762960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- YouBandit ---\n",
      "Final CTR estimates (averaged over all users):\n",
      " - 'Viral thing': 34.51%\n",
      " - 'AI tutorial': 23.08%\n",
      " - 'Boring politics debate': 30.56%\n",
      "\n",
      "Average CTR achieved: 33.85%\n",
      "Theoretical maximum average CTR (if we were smart): 65.00%\n"
     ]
    }
   ],
   "source": [
    "belief_table = np.zeros(env.num_actions)\n",
    "action_counts = np.zeros(env.num_actions)\n",
    "\n",
    "epsilon = 0.1\n",
    "num_steps = 2000\n",
    "history_blind_bandit = []\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # A user arrives, they have a context...\n",
    "    context_idx = np.random.randint(0, env.num_contexts)\n",
    "    \n",
    "    # ...but our agent ignores it! It just uses its single belief table.\n",
    "    if np.random.rand() < epsilon:\n",
    "        action_idx = np.random.randint(0, env.num_actions)\n",
    "    else:\n",
    "        # If multiple actions have the same max belief, this breaks ties randomly\n",
    "        best_action = np.argmax(belief_table)\n",
    "        action_idx = np.random.choice(np.where(belief_table == belief_table[best_action])[0])\n",
    "        \n",
    "    reward = env.get_reward(context_idx, action_idx)\n",
    "    history_blind_bandit.append(reward)\n",
    "    \n",
    "    action_counts[action_idx] += 1 # increment the count\n",
    "    alpha = 1 / action_counts[action_idx] # calculate the 'dynamic learning rate' with the sample average\n",
    "\n",
    "    # Update our table using the incremental average approach we've used in the last lecture\n",
    "    belief_table[action_idx] += alpha * (reward - belief_table[action_idx])     \n",
    "\n",
    "print(\"--- YouBandit ---\")\n",
    "print(\"Final CTR estimates (averaged over all users):\")\n",
    "for i, belief in enumerate(belief_table):\n",
    "    print(f\" - '{env.actions[i]}': {belief:.2%}\")\n",
    "\n",
    "avg_reward = np.mean(history_blind_bandit)\n",
    "print(f\"\\nAverage CTR achieved: {avg_reward:.2%}\")\n",
    "\n",
    "# Optimal policy would be (0.6 for new viewers -> show them the viral thing + 0.7 for the subscribers) / 2 = 65%\n",
    "print(f\"Theoretical maximum average CTR (if we were smart): 65.00%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8fa3f4-747e-40b2-ae7d-8f044138067a",
   "metadata": {},
   "source": [
    "Our simple, state-ignoring agent is failing. Its belief table is kind of ugly, and no where near optimal. It learned that the viral thing is best on average, so it shows this regardless of the user. Therefore, we are annoying our loyal subscribers and probably <span style=\"color:red\">not</span> showing it enough to our new viewers. \n",
    "\n",
    "So how do we approach this?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a3e1726e-dc8a-4756-a153-b4e7052d3add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use 2D table to represent the beliefs\n",
    "belief_table_2d = np.zeros((env.num_contexts,env.num_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8606b114-1040-481f-8dac-ea37446033ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "belief_table_2d # so each row now, correspond to the context, so the first row could be 'new user'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2f70d6c9-29dc-4b55-b15a-869520a0c4a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1814.,   78.,  108.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# need the same for the counts\n",
    "action_counts_2d = np.zeros((env.num_contexts,env.num_actions))\n",
    "action_counts # we can look at the counts of the YouBandit. It thought the viral thing was the best to show, so it showed it to everyone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbdca3e-f4e3-444b-ba52-b74dd3d97af0",
   "metadata": {},
   "source": [
    "\n",
    "##### PersonalBandit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b3e2f1d-8074-4106-a0c0-02b452853366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PersonalBandit ---\n",
      "\n",
      "Final beliefs for new viewers (row 0):\n",
      " - 'Viral thing': 60.63%\n",
      " - 'AI tutorial': 3.45%\n",
      " - 'Boring politics debate': 12.50%\n",
      "\n",
      "Final beliefs for subscribers (row 1):\n",
      " - 'Viral thing': 8.33%\n",
      " - 'AI tutorial': 48.28%\n",
      " - 'Boring politics debate': 71.37%\n",
      "\n",
      "Average CTR achieved: 62.85%\n"
     ]
    }
   ],
   "source": [
    "belief_table_2d = np.zeros((env.num_contexts,env.num_actions))\n",
    "action_counts_2d = np.zeros((env.num_contexts,env.num_actions))\n",
    "\n",
    "history_aware_bandit = []\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # A user arrives with a context\n",
    "    context_idx = np.random.randint(0, env.num_contexts) # pick out one context at random\n",
    "\n",
    "    # ... and then we do our 'sometimes pick a random action'-strategy based on looking up which user has arrived\n",
    "    if np.random.rand() < epsilon:\n",
    "        action_idx = np.random.randint(0, env.num_actions) # with probability ε sometimes pick a random thumbnail to show\n",
    "    else:\n",
    "        action_idx = np.argmax(belief_table_2d[context_idx]) # else, choose the best thumnail according to our belief\n",
    "\n",
    "    reward = env.get_reward(context_idx, action_idx) # pass the thumbnail we want to show to the environment and get the reward back\n",
    "    history_aware_bandit.append((context_idx, action_idx, reward)) # remember which user came (context_idx), what we showed (actions_idx)\n",
    "\n",
    "    # ------------- Update our beliefs ---------------------\n",
    "    action_counts_2d[context_idx, action_idx] += 1 # increment the count for how many times which action we took for which user\n",
    "    belief_table_2d[context_idx, action_idx] += (1/action_counts_2d[context_idx, action_idx]) * (reward - belief_table_2d[context_idx, action_idx])\n",
    "\n",
    "\n",
    "print(\"--- PersonalBandit ---\")\n",
    "print(\"\\nFinal beliefs for new viewers (row 0):\")\n",
    "for i, belief in enumerate(belief_table_2d[0]):\n",
    "    print(f\" - '{env.actions[i]}': {belief:.2%}\")\n",
    "\n",
    "print(\"\\nFinal beliefs for subscribers (row 1):\")\n",
    "for i, belief in enumerate(belief_table_2d[1]):\n",
    "    print(f\" - '{env.actions[i]}': {belief:.2%}\")\n",
    "    \n",
    "avg_reward_aware = np.mean([example[2] for example in history_aware_bandit])\n",
    "print(f\"\\nAverage CTR achieved: {avg_reward_aware:.2%}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c11d64-e0e7-4dc6-a2ae-0749b40f0c14",
   "metadata": {},
   "source": [
    "WOHOOO! This is *way better* than the 35% we got with *YouBandit* that didn't know anything about states, users or context of the problem.\n",
    "\n",
    "By simply giving our agent a separate belief table for each state, it learned an almost perfect personalised strategy, and actually showed content that the user clicked on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29ea797e-d471-4327-a598-b06f0997985b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[927.,  29.,  24.],\n",
       "       [ 48.,  29., 943.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see the history\n",
    "action_counts_2d  # see that we show a lot more 'viral things' to the new users and mostly 'boring politics debates' to subscribers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e9a73dc-c00a-4150-8888-6559d49f9497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: New viewer showed up, we showed: Viral thing and got reward: 0\n",
      "User: Subscriber showed up, we showed: Viral thing and got reward: 0\n",
      "User: New viewer showed up, we showed: Viral thing and got reward: 0\n",
      "User: New viewer showed up, we showed: Viral thing and got reward: 1\n",
      "User: New viewer showed up, we showed: Viral thing and got reward: 0\n",
      "User: New viewer showed up, we showed: Viral thing and got reward: 0\n",
      "User: Subscriber showed up, we showed: Viral thing and got reward: 0\n",
      "User: New viewer showed up, we showed: Viral thing and got reward: 1\n",
      "User: Subscriber showed up, we showed: Viral thing and got reward: 0\n",
      "User: Subscriber showed up, we showed: Viral thing and got reward: 0\n"
     ]
    }
   ],
   "source": [
    "# Let's take a loot at what users came, what we showed and if the clicked or not\n",
    "for context_idx, action_idx, reward in history_aware_bandit[5:15]:\n",
    "    print(f\"User: {'Subscriber' if context_idx == 1 else 'New viewer'} showed up, we showed: {env.actions[action_idx]} and got reward: {reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d074834f-4242-417f-8f95-b564a8140246",
   "metadata": {},
   "source": [
    "\n",
    "#### Bonus: feeling the pain of an exploding table\n",
    "\n",
    "We are exited, and want to have even more context. Therefore, we want to personalise with *everything* we can think of in our imagination. We'll personalise with *device type*, *country*, *time of day*. Notice that what we are doing now is <span style=\"color:purple\">growing</span> our <span style=\"color:blue\">state</span> space.\n",
    "\n",
    "Exercise: How many rows do we need now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb758a7f-0517-41c0-9c47-77d107eedc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_user_types = 2\n",
    "num_devices = 3     # assume mobile, desktop, tablet\n",
    "num_countries = 20  # just assume 20 countries\n",
    "num_time_of_day = 3 # assume that we use morning, afternoon, evening\n",
    "num_actions = env.num_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "edaab9a2-fc22-4d7c-95ec-a332cd0d59ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total unique contexts are now: 360\n"
     ]
    }
   ],
   "source": [
    "# total unique contexts is the product of all context variables\n",
    "total_unique_contexts = num_user_types * num_devices * num_countries * num_time_of_day\n",
    "print(f\"The total unique contexts are now: {total_unique_contexts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f3d6eae2-b340-4d3e-b513-7fbd05f563d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our belief table would need the shape: (360, 3)\n"
     ]
    }
   ],
   "source": [
    "table_shape = (total_unique_contexts, num_actions)\n",
    "print(f\"Our belief table would need the shape: {table_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdc0f33-3a69-4a11-9c24-d51ed051c296",
   "metadata": {},
   "source": [
    "This simple extension reveals two lurking <span style=\"color:red\">problems</span>:\n",
    "1. Data sparisty: We need thousands of views to get a good estimate on just **one row** (e.g., for subscripers on a tablet in France in the evening, for example). It could take years to show enough thumbnails to reliably fill out all 360 rows. Most of our rows could remain empty or have really unreliable estimates :(\n",
    "2. No generalisation: Let's say our agent learns that 'Mobile users in the US' love viral videos. The knowledge is stuck on that one specific row of the table. It does not kind of *leak* out so that it helps when a 'Tablet users in the US' shows up for the first time. The agent can't see this pattern. It has to learn everything from scratch for every single context.\n",
    "\n",
    "---\n",
    "This is the wall. Our simple, intuitive solution of creating **a separate belief table for every situation collapses** under *real-world complexity*.\n",
    "\n",
    "The <span style=\"color:red\">core problem</span> is that our brain works by <span style=\"color:Violet\">memorisation</span>. What we need is something that can generalise. We don't want to store separate values for *here* and *one millimeter to the right*. The value we get from these two positions, *should be pretty similar*. And therefore, we need something that understand that. In other words, we need something that understands the relationship between context and results. A brain that could learn that the feature 'Device=Mobile' generally leads to a preference for shorter content, regardless of the country or the time of day, would be more like what we want.\n",
    "\n",
    "Next lecture we will see how to replace our giant table, with machine learning (ML).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715eefd8-a0c3-4366-b8af-f68f47efcdf7",
   "metadata": {},
   "source": [
    "\n",
    "#### Bonus 2: Gym API \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66843588-5784-4569-a821-7d9740ef2af0",
   "metadata": {},
   "source": [
    "So far, I have deliberately held this very high level, hands-on and with very simple examples. Now I want to show you the first step to becoming an RL researcher, by showing you a paper released in 2016 which introduced a toolkit for reinforcement learning. \n",
    "\n",
    "[OpenaI Gym paper](https://arxiv.org/pdf/1606.01540)\n",
    "\n",
    "\n",
    "[OpenAI Gym](https://gymnasium.farama.org/index.html) is a toolkit for reinforcement learning. It includes a collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. \n",
    "\n",
    "\n",
    "##### Mapping our simple homepage environment to the Gymnasium API\n",
    "Let's create a clear side-by-side comparison:\n",
    "\n",
    "| Gymnasium API (env.step) | Our HomepageEnv     | Explanation                                                                 |\n",
    "|---------------------------|----------------------------|-----------------------------------------------------------------------------|\n",
    "| observation (next state)  | Doesn't Exist              | Because our episode ends after one step, there is no \"next state\" to transition to. |\n",
    "| reward                    | The 1 or 0 from get_reward() | This is a direct one-to-one mapping. This is the core feedback signal.       |\n",
    "| terminated                | Always True                | Our \"episode\" always terminates after a single action.                       |\n",
    "| truncated                 | Always False               | Our episode can't be cut short; it's already as short as possible.           |\n",
    "| info                      | Doesn't Exist              | We don't need extra debugging information.                                   |\n",
    "\n",
    "The `env.reset()` in Gymnasium gives you the first observation of an episode.  \n",
    "In our case, this is equivalent to *“a new user arrives.”*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e18b4f3-9818-491d-840b-8bc2075dbbe1",
   "metadata": {},
   "source": [
    "\n",
    "##### Rewrite our environment to the Gym API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ce41bc5c-cfed-47ad-87a2-c0e0d0982e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: This version contains a subtle bug, which will complicate the learning process. \n",
    "class GymHomepageEnv:\n",
    "    def __init__(self):\n",
    "        self.contexts = [\"New viewer\", \"Subscriber\"]\n",
    "        self.actions = [\"🔥 Viral thing\", \"🤖 AI tutorial\", \"💤 Boring politics debate\"]\n",
    "        self.num_contexts = len(self.contexts)\n",
    "        self.num_actions = len(self.actions)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Called at the start of a new episode (a new user arrives).\"\"\"\n",
    "        # Returns the first observation (the user's context)\n",
    "        context_idx = np.random.randint(0, self.num_contexts)\n",
    "        info = {} # empty info dict\n",
    "        return context_idx, info\n",
    "\n",
    "    def step(self, action_idx):\n",
    "        \"\"\"Called when an agent takes an action.\"\"\"\n",
    "        # For this to work, we need to know which context we are in.\n",
    "        # A real Gym env would store this internally, but we'll assume it's known.\n",
    "        # Let's say the last context from reset() was stored\n",
    "        # (This is a bit of a hack to fit our problem into the API)\n",
    "\n",
    "        # In a real scenario, the context would be part of the env's internal state.\n",
    "        # Here, we just cenerate it on the fly for the example\n",
    "        current_context = np.random.randint(0, self.num_contexts)  # <-- !!!! BUG !!!!\n",
    "\n",
    "        # Get the reward using the logic from our simple env\n",
    "        if current_context == 0: # a new viewer is watching\n",
    "            true_probs = [0.6, 0.1, 0.05]\n",
    "        else:                    # subscriber who is thoughful and enjoys politics debate\n",
    "            true_probs = [0.1, 0.5, 0.7]\n",
    "        reward = 1 if np.random.rand() < true_probs[action_idx] else 0\n",
    "\n",
    "        # Since the episode is always over, there is no \"next_observation\"\n",
    "        # and terminated is always True\n",
    "        next_observation = None\n",
    "        terminated = True\n",
    "        truncated = False\n",
    "        info = {}\n",
    "\n",
    "        return next_observation, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9f892e8b-b7bf-49a4-99c0-8fba4b42ba97",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GymHomepageEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb50d0a9-24e8-4798-960f-a32c3873b66a",
   "metadata": {},
   "source": [
    "\n",
    "##### Agent would now interact with the work using env.step(), and env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2aa27870-dead-4edd-ba4d-8d36e0a9ab4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "belief_table_2d = np.zeros((env.num_contexts,env.num_actions))\n",
    "action_counts_2d = np.zeros((env.num_contexts,env.num_actions))\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # 1. A new episode starts (a user arrives)\n",
    "    # env.reset() gives us the context for THIS episode\n",
    "    context_idx, info = env.reset() # replaces our old 'context_idx = np.random.randint(...)'\n",
    "\n",
    "    # 2. Agent makes a descisino based on the context\n",
    "    if np.random.rand() < epsilon:\n",
    "        action_idx = np.random.randint(0, num_actions)\n",
    "    else:\n",
    "        action_idx = np.argmax(belief_table_2d[context_idx])\n",
    "\n",
    "    # 3. Agent takes a step in the environment\n",
    "    # note: in our specific case, the step function does not use the info of which user is in context\n",
    "    # which is a flaw in this analogy, but highlights that our problem is not truly sequential\n",
    "    # In a real Gym env, the state from reset() would be used by step()\n",
    "    observation, reward, terminated, truncated, info = env.step(action_idx)\n",
    "\n",
    "    # 4. Agent learns from the experience\n",
    "    action_counts_2d[context_idx, action_idx] += 1\n",
    "    belief_table_2d[context_idx, action_idx] += 1 / (action_counts_2d[context_idx, action_idx]) * (reward - belief_table_2d[context_idx, action_idx])\n",
    "\n",
    "    # 5. Check if the episode is over (it always is for us, because we are working with one-step MDPs)\n",
    "    if terminated or truncated:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ababe791-e943-4b8e-a050-6ef67a6c66e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[110., 177., 684.],\n",
       "       [308.,  49., 672.]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "belief_table_2d\n",
    "action_counts_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb99401c-c11e-4a03-a511-208af7ecf30e",
   "metadata": {},
   "source": [
    "\n",
    "##### Our subtle bug: debugging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c33015-7fe3-4c24-9460-7a4f35a1eee2",
   "metadata": {},
   "source": [
    "1. The agent starts a new episode. env.reset() tells it, \"The user is a Subscriber (context_idx = 1).\"\n",
    "2. The agent looks at its brain for subscribers and correctly decides to show the \"Community Q&A\" video (action_idx = 2).\n",
    "3. The agent calls env.step(action_idx=2).\n",
    "4. Inside env.step, our faulty code ignores the fact that the user is a subscriber. It randomly generates a new context: current_context = np.random.randint(0, 2). Let's say it randomly picks 0 (\"New Viewer\").\n",
    "5. The environment then calculates the reward based on a New Viewer seeing the Community Q&A. The CTR is only 5%, so the reward is almost certainly 0.\n",
    "6. The env.step function returns this reward=0 back to the agent.\n",
    "7. The agent, still thinking it's dealing with a Subscriber, takes this reward=0 and updates its belief for belief_table_2d[1, 2]. It wrongly concludes that \"Community Q&A for Subscribers\" is a bad idea.\n",
    "\n",
    "\n",
    "The reward signal is being completely mismatched from the context. The agent is being told about one situation but is being graded on a totally different one. This injects so much noise into the learning process that the agent can't figure out the true values, leading to the confused action_counts you observed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfee519f-8655-44f7-832e-ec3a858ca388",
   "metadata": {},
   "source": [
    "So, basically the <span style=\"color:blue\">state</span> is decoupled from the reward. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ae5ef8-fbb2-4172-9c6c-b51dafe9b8c2",
   "metadata": {},
   "source": [
    "This is a classic RL debugging scenario. Our agent's performance is terrible, and the action counts show it isn't learning the right policy. So where is the bug? Is it in the environment or in the brain we've built?\n",
    "\n",
    "Let's trace the flow of information for a single step:\n",
    "1. **Inside `step`**: Here's the bug! The `step` function completely **ignores** the fact that the user was a subscriber (context_idx =1). It re-rolls the dice with `current_context = np.random.randint(...)`. It might decide the reward should be based on a \"new viewer\" (context_idx=0)\n",
    "5.  **Corrupted reward signal**: The agent chose an action based on its *beliefs about subscribers*, but it *received a reward that might have come from a new viewer's experience*. The feedback signal is now corrupted. The agent is being punished or rewarded for the wrong reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4df94a-fada-414d-81c8-c70d8ab8c512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrected version: Gym-like version of our environment\n",
    "class GymHomepageEnv:\n",
    "    def __init__(self):\n",
    "        self.contexts = [\"New viewer\", \"Subscriber\"]\n",
    "        self.actions = [\"🔥 Viral thing\", \"🤖 AI tutorial\", \"💤 Boring politics debate\"]\n",
    "        self.num_contexts = len(self.contexts)\n",
    "        self.num_actions = len(self.actions)\n",
    "        \n",
    "        # The environment now has an internal memory of the current state\n",
    "        self.current_context_idx = None \n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"A new user arrives. The environment sets its internal state and returns it as an observation.\"\"\"\n",
    "        self.current_context_idx = np.random.randint(0, self.num_contexts)\n",
    "        info = {}\n",
    "        return self.current_context_idx, info\n",
    "\n",
    "    def step(self, action_idx):\n",
    "        \"\"\"The agent takes an action. The environment uses its INTERNAL state to calculate the outcome.\"\"\"\n",
    "        if self.current_context_idx is None:\n",
    "            raise Exception(\"Cannot call step before reset. Start a new episode.\")\n",
    "\n",
    "        # THE BUG IS FIXED: We use the stored internal state, not a new random one.\n",
    "        if self.current_context_idx == 0:  # new viewer\n",
    "            true_probs = [0.6, 0.1, 0.05]\n",
    "        else:                              # thoughtful subscriber\n",
    "            true_probs = [0.1, 0.5, 0.7]\n",
    "        \n",
    "        reward = 1 if np.random.rand() < true_probs[action_idx] else 0\n",
    "        \n",
    "        # After the step, the episode is over, so we can clear the internal state\n",
    "        # until the next reset.\n",
    "        self.current_context_idx = None\n",
    "        \n",
    "        next_observation = None \n",
    "        terminated = True\n",
    "        truncated = False\n",
    "        info = {}\n",
    "        \n",
    "        return next_observation, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b822404a-b406-4a64-9782-2d504a6e7831",
   "metadata": {},
   "source": [
    "We've just peeked behind the curtain and seen that the tools we use are built on a deep understanding of the challenges of RL research.\n",
    "\n",
    "- We need freedom to design agents ('Environments, not Agents')\n",
    "- We need our results to be comparable ('Strict versioning')\n",
    "- We need to learn effectively ('Sample complexity)\n",
    "\n",
    "We now are in the perfect position to see the limitations of our work so far. Our tabular methods had terrible *sample complexity* (i.e., they needed too much data) and they *couldn't generalise* (everything was hidden in the 'boxes' of our  tabels, with no leak between the different states)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
