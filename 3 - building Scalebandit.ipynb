{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a29c3e4b-8560-4f13-8e03-1d6f85b40ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ed4652-3741-4f0d-801d-1094febeca86",
   "metadata": {},
   "source": [
    "Hi everyone! Today we are building *Scalebandit*, which will address the problems we encountered in the last lecture.\n",
    "\n",
    "If you remember back to Lecture 2, we actually made *personalised content* by introducing the concept of <span style=\"color:blue\">states</span>. We now had a row for each different type of user that we wanted to make the best YouTube homepage for. We also remember that this worked perfectly, until we tried to scale it. We discovered what is called the **curse of dimensionality**. As we got exited and added more <span style=\"color:blue\">states</span>, our table blew up, because we had to have a single cell, for each new 'thing' we added, and if multiple things are going to have their own cell, then the total number of cells is all these things multiplied together. So the table exploded in size, becoming hard to fit into our computer memory, and also was really hard to get anything meaningful out of, because of the fact that we needed to actually have insane amounts of data to be sure that we saw each cell, for each specific case (tablet user in France at midnight), several times so our estmiate of that value became something that we could trust. \n",
    "\n",
    "This is the wall that the table methods hit. The brain works kind of by **memorisation**. It *needs a separate memory slot (a row in our table) for every single possible situation*. \n",
    "\n",
    "Today, we are going to solve this problem by trying to find a method that can **generalise** (i.e., say anything about things that it has never encountered before). \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f30b230-c749-4620-82ec-9d8e47066a55",
   "metadata": {},
   "source": [
    "Think about when you learned the gangetabell. One day your teacher said: \"tomorrow I'm going to test you with numbers from 0 to 10, so you need to learn this\". You go home and you write down a large 2D-table for all numbers that can multiply (e.g., 2 x 3 = 6, 4 x 5 = 20 etc.). So now each answer has it's own cell. Then you try rally hard to hold your hand over every cell and try again and again to remember what the answer is, for each combination of numbers, until you get it absolutely perfect, and you go with a big dick walk to school the next day. You know *all the answers*. This is a bit like what we are doing with our table in the last lecture. We had our table, and if you wanted to know the value of showing a viral thing to a new viewer, you just looked it up: `belief_table[0, 0]`. If the entry wasn't there, you had no idea of what the value was. \n",
    "\n",
    "However, there is another way. Imagine you instead built a device, that for any two numbers you put in, gave you the answer. So now when the teacher gave you the numbers 4 and 5, you used your device it gave you 20 back, which you passed on as your answer to the teacher.\n",
    "\n",
    "This is exactly the conceptual change we are going to make today. We **want something that takes our state and gives back the value for showing each different thumbnail**. We don't want to remember it, we want to be able to calculate it on the fly. Even for states we haven't seen before.\n",
    "\n",
    "Formally, we can say that we want: `values = f(state)`\n",
    "\n",
    "Let's give this <span style=\"color:LightCoral\">function</span> a name, so that we can talk about it. Let's call it our *Value function approximator*. Because we want the value, we use a device called a function to return it to us, and approximator because we give the thing a little slack, so that it doesn't have to be perfectly accurate at first, but we can kind of make a reasonable guess for *any* state, even ones we haven't seen before (and how could that be accurate?). This whole process, if we are able to be more and more accurate, is known as **generalisation**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db95f620-793e-45bd-9df5-292acb599329",
   "metadata": {},
   "source": [
    "\n",
    "#### Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ef19b9d-e8e6-4e7b-b31a-3678902828c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HomepageEnv:\n",
    "    def __init__(self):\n",
    "        self.contexts = [\"New viewer\", \"Subscriber\"]\n",
    "        self.actions = [\"ðŸ”¥ Viral thing\", \"ðŸ¤– AI tutorial\", \"ðŸ’¤ Boring politics debate\"]\n",
    "        self.num_contexts = len(self.contexts)\n",
    "        self.num_actions = len(self.actions)\n",
    "\n",
    "    def get_reward(self, context_idx, action_idx):\n",
    "        # New viewers (context 0) are attracted by viral videos\n",
    "        if context_idx == 0: # the index for a new viewer\n",
    "            true_probs = [0.6, 0.1, 0.05] # gives the size of a spinning wheel, here \"viral thing\" has 60% of the space \n",
    "            return 1 if np.random.rand() < true_probs[action_idx] else 0 # like spinning a wheel that can land anywhere from 0.00 to 0.99\n",
    "        # Subscribers (context 1) want deeper content\n",
    "        elif context_idx == 1:\n",
    "            true_probs = [0.1, 0.5, 0.7] # The subscribers are thoughtfull and use YouTube a bit differently, they want boring debates\n",
    "            return 1 if np.random.rand() < true_probs[action_idx] else 0\n",
    "\n",
    "# Initialize environment\n",
    "env = HomepageEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb88e360-d92a-4196-ac96-555de47ab5bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['New viewer', 'Subscriber']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27618f48-f157-4f86-96ce-47fa3235419e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ðŸ”¥ Viral thing', 'ðŸ¤– AI tutorial', 'ðŸ’¤ Boring politics debate']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257ce651-1e06-43da-b3ee-bd256dfa54be",
   "metadata": {},
   "source": [
    "\n",
    "#### Building: the device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cc01897-2e57-4a86-8520-9608dde538c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what's the simplest possible function that we can build? Let's try just a linear thing\n",
    "# So, let's represent our context as a list\n",
    "new_viewer = [1, 0] # just a one representing the type of person that has arrived at our YouTube page\n",
    "subscriber = [0, 1] # the same for this type of user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8db8a19-24f2-410f-98de-10b37002355c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.98827504,  0.99093931, -1.42083327],\n",
       "       [ 0.63843579, -0.07698662, -0.68891338]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What do we miss now? If you remember back to our machine learning class, we actually know how to build this\n",
    "w = np.random.randn(env.num_contexts, env.num_actions)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46cfa26a-ece3-456b-bdb4-8a7627f442ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.98827504,  0.99093931, -1.42083327])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now estiamte the value, by taking our weights and multiply them with our featues\n",
    "np.array(new_viewer) @ w     # (1, 2) x (2, 3) -> (1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24d286db-54e7-4468-9dcd-117bd891d00f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.63843579, -0.07698662, -0.68891338])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In the same way, we can estimate the values from our subscribers\n",
    "np.array(subscriber) @ w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee51220a-2911-4549-b2fb-f1b6d06cefd2",
   "metadata": {},
   "source": [
    "So, by estimating our values in this manner, you see that we 'pluck out' the rows for that type of person we are actually interested in, by dot-producting with a feature vector that is 1, if this is the user we are about, and 0 otherwise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9352c317-c607-4050-9d79-1cbc9025602b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so all that we need from our state representation is now \n",
    "state = np.zeros(env.num_contexts)\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "125aa788-e580-49db-992a-1e6ff78401c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so, a user shows up\n",
    "np.random.randint(0, env.num_contexts) # get a random number between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfc279a7-adc4-426f-affe-618b678bbc0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and we set the feature to be 1, for this type of user that has showed up\n",
    "state[np.random.randint(0, env.num_contexts)] = 1\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7e7053b8-1e7c-469e-ac34-031ac84b04e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.63579846,  0.39491446, -1.471798  ])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now to get the estimates of what we should how to this user, we can just get the correct row by the dot product\n",
    "correct_row_with_probs = state @ w\n",
    "correct_row_with_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "6f77bf66-245e-4d53-bc40-78cad3920a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(correct_row_with_probs).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01b568b-9b44-4d65-bc1e-07baa5946ef0",
   "metadata": {},
   "source": [
    "Let's forget about all the details for a moment and focus on one **single event**.\n",
    "\n",
    "Imagine we see a new viewer (context = [1, 0]). It uses it's current dials to calcualte the estimates CTRs an gets:\n",
    "y_pred = [0.4, 0.1, 0.2]\n",
    "\n",
    "Based on this, it chooses the best action (as we have seen in the eprevious lecture, with the argmax), so `action_idx = 0` because in the list 0.4 has the highest value, and it is located at index 0. We then show the 'ðŸ”¥ Viral thing' thumbnail and the user clicks! The agent recieves a **real-world result**: `reward = 1`\n",
    "\n",
    "So, now we are faced with a simple fact:\n",
    "\n",
    "1. Our dials was set up such that we thought the click-through rate was 0.4\n",
    "2. the reality was better than this, we got 1.\n",
    "\n",
    "So, the difference between reality and what we thought the reality was: `reality - y_pred = 1 - 0.4 = 0.6`. This is our error.\n",
    "\n",
    "In some sense, it was too pessimistic. How can we adjust our <span style=\"color:purple\">dials</span> (*w*) so that if we find ourself in this exact situation again, our thinking will be more like what we have actually experienced now? I.e., a little bit closer to 1?\n",
    "\n",
    "Well, we actually know this from our ML course! Supervised learning! Gradient descent! WOOOHO :))\n",
    "\n",
    "So, we know that to nudge the weight in a direction that reduces our error, we need to multiply by the gradient of our function approximator. So, what is the gradient? Well, our function approximator is pretty easy since it is just linear in the weights, so we have: y_pred = s @ w -> derror/dw = **s**. So our gradient is just the <span style=\"color:blue\">state</span> list. \n",
    "\n",
    "So, we use supervised learning. So our update rule becomes\n",
    "<div style=\"border: 2px solid #988558; padding: 10px; border-radius: 8px; background-color: #EADDCA;\">\n",
    "<b>w_new </b> <-- old_w + little_bit * error * s\n",
    "</div>\n",
    "\n",
    "\n",
    "And this little bit is called the 'learning rate'. This just mean we don't want to step too far, risking overshooting our target.\n",
    "\n",
    "See that we introduced a new word there: the <span style=\"color:goldenrod\">target</span>. What do we mean by target? Well, the <span style=\"color:aquamarine\">environment</span> gives back sort of a \"reality check\". We just call this reality check *something*, and that is the target. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39410359-5483-4ea9-a89e-1371fb4f5f56",
   "metadata": {},
   "source": [
    "Remember from supervised learning, our estimate of the best click-through-rate (CTR) now fully depends on W. These are, like in supervised learning dials we can turn now, and if we are lucky, potentially get them to some values that is right no matter what. \n",
    "\n",
    "However, we don't know in advanve what these values *should be*, so we need to **train those dials up** to give us the <span style=\"color:green\">right answer</span>. And the *way we do that is where the <span style=\"color:coral\">reinforcement learning</span>* comes in. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66092b0a-ba73-4c46-8831-80ac707ee9df",
   "metadata": {},
   "source": [
    "\n",
    "#### Let's build it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0391fec4-b092-4255-a399-7640e3779ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Scalebandit results ---\n",
      "Final weight matrix (W):\n",
      "[[1. 0. 0.]\n",
      " [0. 0. 1.]]\n",
      "\n",
      "Average CTR achieved: 36.63%\n",
      "\n",
      "Let's test our trained 'weights':\n",
      "Predictions for new viewer: [1. 0. 0.]\n",
      "Predictions for subscriber: [0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "W = np.zeros((env.num_contexts, env.num_actions))\n",
    "\n",
    "epsilon = 0.1\n",
    "num_steps = 20000\n",
    "#alpha = 0.06\n",
    "history_linear = []\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # 1. Input (X): Get context (random person shows up), and represent this as 1 in our state list (feature vector)\n",
    "    context_idx = np.random.randint(0, env.num_contexts)\n",
    "    s = np.zeros(env.num_contexts)\n",
    "    s[context_idx] = 1\n",
    "\n",
    "    # 2. Make a prediction (y_pred) - guess the CTRs for this user - with our weights at this moment\n",
    "    ctr_guess = s @ W\n",
    "\n",
    "    # 3. Choose an action\n",
    "    if np.random.rand() < epsilon:\n",
    "        action_idx = np.random.randint(0, env.num_actions)\n",
    "    else:\n",
    "        action_idx = np.argmax(ctr_guess)\n",
    "\n",
    "    # 4. Find the 'true' label (i.e., y in supervised learning)\n",
    "    reward = env.get_reward(context_idx, action_idx)\n",
    "    history_linear.append((context_idx, action_idx, reward))\n",
    "    target = reward\n",
    "\n",
    "    # 5. Calculate the 'error' for the action we took\n",
    "    y_pred = ctr_guess[action_idx]   # based on our predicted ctrs, we choose an action\n",
    "    error = target - y_pred           # this generates an error signal - if we were wrong\n",
    "\n",
    "    # 6. Update the weights with gradient descent (known from supervised learning and ML)\n",
    "    W[:, action_idx] += error * s # Remember that the weight that contributed to this action is found in the column 'action_idx'\n",
    "    # so we update the entire column, but s is [0, 1], for example, so we only add to the weight that was active, or contributed\n",
    "    # the rest gets error * 0 = 0 -> old_weight + 0 = old_weight. Simple as that.\n",
    "\n",
    "# --- Analysis ---\n",
    "print(\"--- Scalebandit results ---\")\n",
    "print(f\"Final weight matrix (W):\\n{np.round(W, 2)}\\n\")\n",
    "\n",
    "avg_reward_linear = np.mean([example[2] for example in history_linear])\n",
    "print(f\"Average CTR achieved: {avg_reward_linear:.2%}\")\n",
    "\n",
    "print(\"\\nLet's test our trained 'weights':\")\n",
    "s_new_viewer = np.array([1, 0])\n",
    "s_subscriber = np.array([0, 1])\n",
    "\n",
    "# The learned values should approximate the true CTRs!\n",
    "# True CTRs for New Viewer: [0.6, 0.1, 0.05]\n",
    "# True CTRs for Subscriber: [0.1, 0.5, 0.7]\n",
    "print(f\"Predictions for new viewer: {s_new_viewer @ W}\")\n",
    "print(f\"Predictions for subscriber: {s_subscriber @ W}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd6fb1a-c068-408a-91ee-e7c244a1237c",
   "metadata": {},
   "source": [
    "Woops, that was not good. Click-through is roughly only 37%. That's barely beating *Youbandit* at 33.85% and way off *Personalbandit* at 62.85%. What gives?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f024d8-60f8-4e8b-970f-d8baedd1c4b7",
   "metadata": {},
   "source": [
    "\n",
    "##### Fixing a bug: an agent with no memory\n",
    "--- \n",
    "Scalebandit results with **no** <span style=\"color:purple\">learning rate</span>\n",
    " Final weight matrix (W):\n",
    "\n",
    "[[0. 0. 0.]\n",
    " [1. 0. 0.]]\n",
    "\n",
    "Average CTR achieved: **37.28%**\n",
    "\n",
    "Trained 'weights':\n",
    "\n",
    "- Predictions for new viewer: [0. 0. 0.]\n",
    "- Predictions for subscriber: [1. 0. 0.]\n",
    "\n",
    " ---\n",
    "\n",
    " Scalebandit results **with** <span style=\"color:purple\">learning rate</span>\n",
    "\n",
    "Final weight matrix (W):\n",
    "[[0.45 0.08 0.03]\n",
    " [0.03 0.51 0.71]]\n",
    "\n",
    "Average CTR achieved: **61.61%**\n",
    "\n",
    "Trained 'weights':\n",
    "- Predictions for new viewer: [0.45448043 0.07525964 0.03243021]\n",
    "- Predictions for subscriber: [0.03078805 0.50880286 0.71027821] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ea5dbc-eeda-4e99-b053-4fbd902a9caa",
   "metadata": {},
   "source": [
    "So, why is this? \n",
    "\n",
    "If you think back to the last lecture, we used this \"sample-average\" method to update our belief table. The more we saw an example and got to choose an action, the less extreme the update got. So over time, the updates was very small and incremental in the end. We had this thing `1 / action_counts[action_idx]` before the difference of the reward and the estimated value. We could essentially call this this something like a *learning rate*. This learning rate acts as a built in *memory management system*. Early on, the agent has little experience, so it has a high *learning rate*. It's pretty aggressive, and it learns what it can from new information because it doesn't know any better. Later on (high N), the agent has *a lot of experience* for that state-action pair, so it treats new information as just one more data point, make only a tiny adjustment to its already-confident belief.  \n",
    "\n",
    "Why do we need a learning rate at all? Because it allows our beliefs to 'land' in a way (converge to the true average technically). In the end, you have to decide on something, so you should not jump around too much. A high learning rate is the equivalent of jumping around a lot - never deciding if you think these are the true values or not. \n",
    "\n",
    "---\n",
    "\n",
    "So why didn't we use the same sample average method or 1/N or whatever for *Scalebandit*?\n",
    "\n",
    "The explenation is slightly technical, and the reason is that we wanted to make our *Personalbandit* general. And more general means being able to tackle **all** problems it could potentially encouter. In our YouTube <span style=\"color:#6F8FAF\">**environment**</span> the *hidden secret* never changed. It had the values locked down. They were written in stone so to speak. Never, ever changing. However, most real-world problems are not like that. They are what is called as **<span style=\"color:red\">non-</span>stationary**. What if user preferences of what thumbnails they like to see change over time (e.g., a viral video because old news, so it is not interesting longer)? Or maybe you play a game, the opponent change their strategy. We want to be able to solve all kinds of problems with our method. And using the sample average method, having your *learning rate* decay to near-zero is not beneficial. Then you never update yourself! You just run some trajectories, then \"Yepp, OKAY, now I know everything I ever need to know\". So we want somethign that is more general, and a common approach in modern RL is to use a constant, but quite small, *learning rate*.\n",
    "\n",
    "This make it so that the agent can \"forget\" old, outdated information and constantly <span style=\"color:#00A36C\">adapt</span> to new realities, or what videos are viral at any point, at least over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fa4825b5-4547-48ae-93e9-33ebd2391824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's expand our error not using a learning rate and just see if this create a running average\n",
    "#W[:, action_idx] += (target - prediction) * s\n",
    "W[:, action_idx] += (1 - (s @ W[:, action_idx])) * s # toggle the reward between 0 and 1 to see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "20b89ca9-34de-4f01-9a21-ccf5bb6897ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0.])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W[:, action_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48d595a-ca6c-4c7b-84ef-d20ded7fd095",
   "metadata": {},
   "source": [
    "\n",
    "##### learning from single samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486e6673-4719-4b5a-9c8c-1389c3b917a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets make this tangible. we are going to zoom in on a single part of our agents brain: the weight `W[0, 0]'\n",
    "# what does this weight represent? It is responsible for the prediction (\"new_viewer\", \"viral thing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b2e3fe5c-6e69-4c5e-bf16-6e3328fb5bf1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ipywidgets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mipywidgets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwidgets\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m display\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ipywidgets'"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8602e32c-3d05-47f3-8a8c-65bff5703b4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a67abcf8-d231-46fe-bdb6-c02bf360daf7",
   "metadata": {},
   "source": [
    "\n",
    "##### A lookup table is a special form of linear function approximation\n",
    "\n",
    "Let's show this relationship. The idea is pretty huge, because it birdges to what we did last lecture. It means we didn't **throw away our old method**, we <span style=\"color:#088F8F\">generalized it</span>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8d8163-0f75-4a66-a3f0-5edc01bb6f03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
