{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a29c3e4b-8560-4f13-8e03-1d6f85b40ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ed4652-3741-4f0d-801d-1094febeca86",
   "metadata": {},
   "source": [
    "Hi everyone! Today we are building *Scalebandit*, which will address the problems we encountered in the last lecture.\n",
    "\n",
    "If you remember back to Lecture 2, we actually made *personalised content* by introducing the concept of <span style=\"color:blue\">states</span>. We now had a row for each different type of user that we wanted to make the best YouTube homepage for. We also remember that this worked perfectly, until we tried to scale it. We discovered what is called the **curse of dimensionality**. As we got exited and added more <span style=\"color:blue\">states</span>, our table blew up, because we had to have a single cell, for each new 'thing' we added, and if multiple things are going to have their own cell, then the total number of cells is all these things multiplied together. So the table exploded in size, becoming hard to fit into our computer memory, and also was really hard to get anything meaningful out of, because of the fact that we needed to actually have insane amounts of data to be sure that we saw each cell, for each specific case (tablet user in France at midnight), several times so our estmiate of that value became something that we could trust. \n",
    "\n",
    "This is the wall that the table methods hit. The brain works kind of by **memorisation**. It *needs a separate memory slot (a row in our table) for every single possible situation*. \n",
    "\n",
    "Today, we are going to solve this problem by trying to find a method that can **generalise** (i.e., say anything about things that it has never encountered before). \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f30b230-c749-4620-82ec-9d8e47066a55",
   "metadata": {},
   "source": [
    "Think about when you learned the gangetabell. One day your teacher said: \"tomorrow I'm going to test you with numbers from 0 to 10, so you need to learn this\". You go home and you write down a large 2D-table for all numbers that can multiply (e.g., 2 x 3 = 6, 4 x 5 = 20 etc.). So now each answer has it's own cell. Then you try rally hard to hold your hand over every cell and try again and again to remember what the answer is, for each combination of numbers, until you get it absolutely perfect, and you go with a big dick walk to school the next day. You know *all the answers*. This is a bit like what we are doing with our table in the last lecture. We had our table, and if you wanted to know the value of showing a viral thing to a new viewer, you just looked it up: `belief_table[0, 0]`. If the entry wasn't there, you had no idea of what the value was. \n",
    "\n",
    "However, there is another way. Imagine you instead built a device, that for any two numbers you put in, gave you the answer. So now when the teacher gave you the numbers 4 and 5, you used your device it gave you 20 back, which you passed on as your answer to the teacher.\n",
    "\n",
    "This is exactly the conceptual change we are going to make today. We **want something that takes our state and gives back the value for showing each different thumbnail**. We don't want to remember it, we want to be able to calculate it on the fly. Even for states we haven't seen before.\n",
    "\n",
    "Formally, we can say that we want: `values = f(state)`\n",
    "\n",
    "Let's give this <span style=\"color:LightCoral\">function</span> a name, so that we can talk about it. Let's call it our *Value function approximator*. Because we want the value, we use a device called a function to return it to us, and approximator because we give the thing a little slack, so that it doesn't have to be perfectly accurate at first, but we can kind of make a reasonable guess for *any* state, even ones we haven't seen before (and how could that be accurate?). This whole process, if we are able to be more and more accurate, is known as **generalisation**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db95f620-793e-45bd-9df5-292acb599329",
   "metadata": {},
   "source": [
    "\n",
    "#### Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ef19b9d-e8e6-4e7b-b31a-3678902828c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HomepageEnv:\n",
    "    def __init__(self):\n",
    "        self.contexts = [\"New viewer\", \"Subscriber\"]\n",
    "        self.actions = [\"ðŸ”¥ Viral thing\", \"ðŸ¤– AI tutorial\", \"ðŸ’¤ Boring politics debate\"]\n",
    "        self.num_contexts = len(self.contexts)\n",
    "        self.num_actions = len(self.actions)\n",
    "\n",
    "    def get_reward(self, context_idx, action_idx):\n",
    "        # New viewers (context 0) are attracted by viral videos\n",
    "        if context_idx == 0: # the index for a new viewer\n",
    "            true_probs = [0.6, 0.1, 0.05] # gives the size of a spinning wheel, here \"viral thing\" has 60% of the space \n",
    "            return 1 if np.random.rand() < true_probs[action_idx] else 0 # like spinning a wheel that can land anywhere from 0.00 to 0.99\n",
    "        # Subscribers (context 1) want deeper content\n",
    "        elif context_idx == 1:\n",
    "            true_probs = [0.1, 0.5, 0.7] # The subscribers are thoughtfull and use YouTube a bit differently, they want boring debates\n",
    "            return 1 if np.random.rand() < true_probs[action_idx] else 0\n",
    "\n",
    "# Initialize environment\n",
    "env = HomepageEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb88e360-d92a-4196-ac96-555de47ab5bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['New viewer', 'Subscriber']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27618f48-f157-4f86-96ce-47fa3235419e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ðŸ”¥ Viral thing', 'ðŸ¤– AI tutorial', 'ðŸ’¤ Boring politics debate']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257ce651-1e06-43da-b3ee-bd256dfa54be",
   "metadata": {},
   "source": [
    "\n",
    "#### Building: the device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cc01897-2e57-4a86-8520-9608dde538c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what's the simplest possible function that we can build? Let's try just a linear thing\n",
    "# So, let's represent our context as a list\n",
    "new_viewer = [1, 0] # just a one representing the type of person that has arrived at our YouTube page\n",
    "subscriber = [0, 1] # the same for this type of user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8db8a19-24f2-410f-98de-10b37002355c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.91990729,  0.76001969,  0.34067624],\n",
       "       [-0.09454043,  0.49068807,  0.67292079]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What do we miss now? If you remember back to our machine learning class, we actually know how to build this\n",
    "w = np.random.randn(env.num_contexts, env.num_actions)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46cfa26a-ece3-456b-bdb4-8a7627f442ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.91990729,  0.76001969,  0.34067624])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now estiamte the value, by taking our weights and multiply them with our featues\n",
    "np.array(new_viewer) @ w     # (1, 2) x (2, 3) -> (1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24d286db-54e7-4468-9dcd-117bd891d00f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.09454043,  0.49068807,  0.67292079])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In the same way, we can estimate the values from our subscribers\n",
    "np.array(subscriber) @ w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee51220a-2911-4549-b2fb-f1b6d06cefd2",
   "metadata": {},
   "source": [
    "So, by estimating our values in this manner, you see that we 'pluck out' the rows for that type of person we are actually interested in, by dot-producting with a feature vector that is 1, if this is the user we are about, and 0 otherwise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9352c317-c607-4050-9d79-1cbc9025602b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so all that we need from our state representation is now \n",
    "state = np.zeros(env.num_contexts)\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "125aa788-e580-49db-992a-1e6ff78401c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so, a user shows up\n",
    "np.random.randint(0, env.num_contexts) # get a random number between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfc279a7-adc4-426f-affe-618b678bbc0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and we set the feature to be 1, for this type of user that has showed up\n",
    "state[np.random.randint(0, env.num_contexts)] = 1\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7e7053b8-1e7c-469e-ac34-031ac84b04e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.63579846,  0.39491446, -1.471798  ])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now to get the estimates of what we should how to this user, we can just get the correct row by the dot product\n",
    "correct_row_with_probs = state @ w\n",
    "correct_row_with_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "6f77bf66-245e-4d53-bc40-78cad3920a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(correct_row_with_probs).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01b568b-9b44-4d65-bc1e-07baa5946ef0",
   "metadata": {},
   "source": [
    "Let's forget about all the details for a moment and focus on one **single event**.\n",
    "\n",
    "Imagine we see a new viewer (context = [1, 0]). It uses it's current dials to calcualte the estimates CTRs an gets:\n",
    "y_pred = [0.4, 0.1, 0.2]\n",
    "\n",
    "Based on this, it chooses the best action (as we have seen in the eprevious lecture, with the argmax), so `action_idx = 0` because in the list 0.4 has the highest value, and it is located at index 0. We then show the 'ðŸ”¥ Viral thing' thumbnail and the user clicks! The agent recieves a **real-world result**: `reward = 1`\n",
    "\n",
    "So, now we are faced with a simple fact:\n",
    "\n",
    "1. Our dials was set up such that we thought the click-through rate was 0.4\n",
    "2. the reality was better than this, we got 1.\n",
    "\n",
    "So, the difference between reality and what we thought the reality was: `reality - y_pred = 1 - 0.4 = 0.6`. This is our error.\n",
    "\n",
    "In some sense, it was too pessimistic. How can we adjust our <span style=\"color:purple\">dials</span> (*w*) so that if we find ourself in this exact situation again, our thinking will be more like what we have actually experienced now? I.e., a little bit closer to 1?\n",
    "\n",
    "Well, we actually know this from our ML course! Supervised learning! Gradient descent! WOOOHO :))\n",
    "\n",
    "So, we know that to nudge the weight in a direction that reduces our error, we need to multiply by the gradient of our function approximator. So, what is the gradient? Well, our function approximator is pretty easy since it is just linear in the weights, so we have: y_pred = s @ w -> derror/dw = **s**. So our gradient is just the <span style=\"color:blue\">state</span> list. \n",
    "\n",
    "So, we use supervised learning. So our update rule becomes\n",
    "<div style=\"border: 2px solid #988558; padding: 10px; border-radius: 8px; background-color: #EADDCA;\">\n",
    "<b>w_new </b> <-- old_w + little_bit * error * s\n",
    "</div>\n",
    "\n",
    "\n",
    "And this little bit is called the 'learning rate'. This just mean we don't want to step too far, risking overshooting our target.\n",
    "\n",
    "See that we introduced a new word there: the <span style=\"color:goldenrod\">target</span>. What do we mean by target? Well, the <span style=\"color:aquamarine\">environment</span> gives back sort of a \"reality check\". We just call this reality check *something*, and that is the target. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39410359-5483-4ea9-a89e-1371fb4f5f56",
   "metadata": {},
   "source": [
    "Remember from supervised learning, our estimate of the best click-through-rate (CTR) now fully depends on W. These are, like in supervised learning dials we can turn now, and if we are lucky, potentially get them to some values that is right no matter what. \n",
    "\n",
    "However, we don't know in advanve what these values *should be*, so we need to **train those dials up** to give us the <span style=\"color:green\">right answer</span>. And the *way we do that is where the <span style=\"color:coral\">reinforcement learning</span>* comes in. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66092b0a-ba73-4c46-8831-80ac707ee9df",
   "metadata": {},
   "source": [
    "\n",
    "#### Let's build it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0391fec4-b092-4255-a399-7640e3779ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Scalebandit results ---\n",
      "Final weight matrix (W):\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "\n",
      "Average CTR achieved: 36.45%\n",
      "\n",
      "Let's test our trained 'weights':\n",
      "Predictions for new viewer: [0. 0. 0.]\n",
      "Predictions for subscriber: [0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "W = np.zeros((env.num_contexts, env.num_actions))\n",
    "\n",
    "epsilon = 0.1\n",
    "num_steps = 2000\n",
    "#alpha = 0.06\n",
    "history_linear = []\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # 1. Input (X): Get context (random person shows up), and represent this as 1 in our state list (feature vector)\n",
    "    context_idx = np.random.randint(0, env.num_contexts)\n",
    "    s = np.zeros(env.num_contexts)\n",
    "    s[context_idx] = 1\n",
    "\n",
    "    # 2. Make a prediction (y_pred) - guess the CTRs for this user - with our weights at this moment\n",
    "    ctr_guess = s @ W\n",
    "\n",
    "    # 3. Choose an action\n",
    "    if np.random.rand() < epsilon:\n",
    "        action_idx = np.random.randint(0, env.num_actions)\n",
    "    else:\n",
    "        action_idx = np.argmax(ctr_guess)\n",
    "\n",
    "    # 4. Find the 'true' label (i.e., y in supervised learning)\n",
    "    reward = env.get_reward(context_idx, action_idx)\n",
    "    history_linear.append((context_idx, action_idx, reward))\n",
    "    target = reward\n",
    "\n",
    "    # 5. Calculate the 'error' for the action we took\n",
    "    y_pred = ctr_guess[action_idx]   # based on our predicted ctrs, we choose an action\n",
    "    error = target - y_pred           # this generates an error signal - if we were wrong\n",
    "\n",
    "    # 6. Update the weights with gradient descent (known from supervised learning and ML)\n",
    "    W[:, action_idx] += error * s # Remember that the weight that contributed to this action is found in the column 'action_idx'\n",
    "    # so we update the entire column, but s is [0, 1], for example, so we only add to the weight that was active, or contributed\n",
    "    # the rest gets error * 0 = 0 -> old_weight + 0 = old_weight. Simple as that.\n",
    "\n",
    "# --- Analysis ---\n",
    "print(\"--- Scalebandit results ---\")\n",
    "print(f\"Final weight matrix (W):\\n{np.round(W, 2)}\\n\")\n",
    "\n",
    "avg_reward_linear = np.mean([example[2] for example in history_linear])\n",
    "print(f\"Average CTR achieved: {avg_reward_linear:.2%}\")\n",
    "\n",
    "print(\"\\nLet's test our trained 'weights':\")\n",
    "s_new_viewer = np.array([1, 0])\n",
    "s_subscriber = np.array([0, 1])\n",
    "\n",
    "# The learned values should approximate the true CTRs!\n",
    "# True CTRs for New Viewer: [0.6, 0.1, 0.05]\n",
    "# True CTRs for Subscriber: [0.1, 0.5, 0.7]\n",
    "print(f\"Predictions for new viewer: {s_new_viewer @ W}\")\n",
    "print(f\"Predictions for subscriber: {s_subscriber @ W}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd6fb1a-c068-408a-91ee-e7c244a1237c",
   "metadata": {},
   "source": [
    "Woops, that was not good. Click-through is roughly only 37%. That's barely beating *Youbandit* at 33.85% and way off *Personalbandit* at 62.85%. What gives?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f024d8-60f8-4e8b-970f-d8baedd1c4b7",
   "metadata": {},
   "source": [
    "\n",
    "##### Fixing a bug: an agent with no memory\n",
    "--- \n",
    "Scalebandit results with **no** <span style=\"color:purple\">learning rate</span>\n",
    " Final weight matrix (W):\n",
    "\n",
    "[[0. 0. 0.]\n",
    " [1. 0. 0.]]\n",
    "\n",
    "Average CTR achieved: **37.28%**\n",
    "\n",
    "Trained 'weights':\n",
    "\n",
    "- Predictions for new viewer: [0. 0. 0.]\n",
    "- Predictions for subscriber: [1. 0. 0.]\n",
    "\n",
    " ---\n",
    "\n",
    " Scalebandit results **with** <span style=\"color:purple\">learning rate</span>\n",
    "\n",
    "Final weight matrix (W):\n",
    "[[0.45 0.08 0.03]\n",
    " [0.03 0.51 0.71]]\n",
    "\n",
    "Average CTR achieved: **61.61%**\n",
    "\n",
    "Trained 'weights':\n",
    "- Predictions for new viewer: [0.45448043 0.07525964 0.03243021]\n",
    "- Predictions for subscriber: [0.03078805 0.50880286 0.71027821] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ea5dbc-eeda-4e99-b053-4fbd902a9caa",
   "metadata": {},
   "source": [
    "So, why is this? \n",
    "\n",
    "If you think back to the last lecture, we used this \"sample-average\" method to update our belief table. The more we saw an example and got to choose an action, the less extreme the update got. So over time, the updates was very small and incremental in the end. We had this thing `1 / action_counts[action_idx]` before the difference of the reward and the estimated value. We could essentially call this this something like a *learning rate*. This learning rate acts as a built in *memory management system*. Early on, the agent has little experience, so it has a high *learning rate*. It's pretty aggressive, and it learns what it can from new information because it doesn't know any better. Later on (high N), the agent has *a lot of experience* for that state-action pair, so it treats new information as just one more data point, make only a tiny adjustment to its already-confident belief.  \n",
    "\n",
    "Why do we need a learning rate at all? Because it allows our beliefs to 'land' in a way (converge to the true average technically). In the end, you have to decide on something, so you should not jump around too much. A high learning rate is the equivalent of jumping around a lot - never deciding if you think these are the true values or not. \n",
    "\n",
    "---\n",
    "\n",
    "So why didn't we use the same sample average method or 1/N or whatever for *Scalebandit*?\n",
    "\n",
    "The explenation is slightly technical, and the reason is that we wanted to make our *Personalbandit* general. And more general means being able to tackle **all** problems it could potentially encouter. In our YouTube <span style=\"color:#6F8FAF\">**environment**</span> the *hidden secret* never changed. It had the values locked down. They were written in stone so to speak. Never, ever changing. However, most real-world problems are not like that. They are what is called as **<span style=\"color:red\">non-</span>stationary**. What if user preferences of what thumbnails they like to see change over time (e.g., a viral video because old news, so it is not interesting longer)? Or maybe you play a game, the opponent change their strategy. We want to be able to solve all kinds of problems with our method. And using the sample average method, having your *learning rate* decay to near-zero is not beneficial. Then you never update yourself! You just run some trajectories, then \"Yepp, OKAY, now I know everything I ever need to know\". So we want somethign that is more general, and a common approach in modern RL is to use a constant, but quite small, *learning rate*.\n",
    "\n",
    "This make it so that the agent can \"forget\" old, outdated information and constantly <span style=\"color:#00A36C\">adapt</span> to new realities, or what videos are viral at any point, at least over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa4825b5-4547-48ae-93e9-33ebd2391824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's expand our error not using a learning rate and just see if this create a running average\n",
    "#W[:, action_idx] += (target - prediction) * s\n",
    "W[:, action_idx] += (1 - (s @ W[:, action_idx])) * s # toggle the reward between 0 and 1 to see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20b89ca9-34de-4f01-9a21-ccf5bb6897ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W[:, action_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48d595a-ca6c-4c7b-84ef-d20ded7fd095",
   "metadata": {},
   "source": [
    "\n",
    "##### learning from single samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0dff24-22e8-487c-9577-d1c1ceec3c81",
   "metadata": {},
   "source": [
    "Let's make this tangible. We will simulate a single learning step for our agent and print out thoughts it has as we go.\n",
    "\n",
    "We'll focus on just one part of our its brain: the weight `W[0, 0]`. This weight is responsible for predicting the click-through rate of the 'ðŸ”¥ Viral thing' (action=0) when shown to a 'new viewer' (state=0).\n",
    "\n",
    "The **ground thruth** (which the agent doen't know, the *hidden secret*) is that the real CTR is 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "487ae0f1-6bf4-4fff-994e-d0bd41e86ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_ctr = 0.6 # from 'true probs' in our HomepageEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db436ff0-f11b-4170-b769-4a8be5d439d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1 # the agents learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb827e01-5087-45a5-b8d9-1c0fefca784e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent's 'starting belief' for 'ðŸ”¥ Viral thing' shown to a new user is: 0.40\n"
     ]
    }
   ],
   "source": [
    "# let's say the agent has some experience, and its current belief for W[0,0] is 0.4 - currently a bit pessimistic\n",
    "current_belief = 0.4\n",
    "print(f\"Agent's 'starting belief' for 'ðŸ”¥ Viral thing' shown to a new user is: {current_belief:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa57a8e-7cc0-41b4-b73d-c72e93bf1237",
   "metadata": {},
   "source": [
    "Now, imagine that we get some experience, a single sample. A \"new viewer\" arrives, we shown them the 'ðŸ”¥ Viral thing', and we just sit back and observe the outcome. Let's simulate how it could play out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8827c5af-eddc-46f3-b1fa-5a16bade16b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A new user arrives...\n",
      "The REALITY of this single experience was: WAHAY! CLICK! ðŸ¤©ðŸ¥³\n"
     ]
    }
   ],
   "source": [
    "# re-run this cell a few times. You'll seea bout 60% of the time you get a reward of 1 and 40% you'll get 0. \n",
    "reward = 1 if np.random.randn() < true_ctr else 0\n",
    "print(f\"A new user arrives...\")\n",
    "print(f\"The REALITY of this single experience was: {'WAHAY! CLICK! ðŸ¤©ðŸ¥³' if reward == 1 else 'No click this time :('}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac736fd1-749a-4375-92b9-e270e074672a",
   "metadata": {},
   "source": [
    "\n",
    "##### the agents \"though process\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92102462-99fc-45a1-9379-64738fbbc44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What did I actually think? Well, I thought the likely reward of a click was about: 0.4\n",
      "The 'target' (i.e., this one reality) was: 1\n",
      "\n",
      "Therefore, I want to adjust my thinking so as to believe that I would be getting 1 all along\n"
     ]
    }
   ],
   "source": [
    "# our agent just saw as single, real-world 'reward'. Now, let's trace how it learns from this single experience\n",
    "prediction = current_belief\n",
    "print(f\"What did I actually think? Well, I thought the likely reward of a click was about: {prediction}\")\n",
    "\n",
    "target = reward\n",
    "print(f\"The 'target' (i.e., this one reality) was: {target}\")\n",
    "print()\n",
    "print(f\"Therefore, I want to adjust my thinking so as to believe that I would be getting {target} all along\")\n",
    "error = target - prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8653f9a-c9cd-43a7-818e-82a168c51d96",
   "metadata": {},
   "source": [
    "How will I update my belief?\n",
    "\n",
    "I'll <span style=\"color:purple\">nudge</span> my <span style=\"color:gray\">old belief</span> in the **direction of the error**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6dd59ad4-1885-41e9-a9fb-d3fa414c7443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nudge the old belief in the direction of the error\n",
    "nudge = lr * error\n",
    "nudge # so the nudge is positive / negative (depending on what happened above): if positive - we want to think \"higher of\" this action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a68b3ed4-d384-4111-9712-ac3d08c7d080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My old thinking was 0.4 likely for this action, however, after seeing a sample of real-world experience (i.e., 1),\n",
      "my new thinking is: 0.46\n"
     ]
    }
   ],
   "source": [
    "# What's my new belief, the agent wonders?\n",
    "# Well, it's what you thought before, pluss the new, updated version of what you experienced in reality\n",
    "new_belief = current_belief  + nudge\n",
    "print(f\"My old thinking was {current_belief} likely for this action, however, after seeing a sample of real-world experience (i.e., {reward}),\\nmy new thinking is: {new_belief}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8395cb5c-1f40-41a8-b100-813b917687b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_belief = new_belief"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058e3cb9-247a-420d-bd67-d8db3495830d",
   "metadata": {},
   "source": [
    "Therefore, we can see that the <span style=\"color:#FF69B4\">next time the agent in the same position again</span>, choosing this thumbnail to show is now a little bit **more likely**, so *it thinks*, based on what *it has experienced in the \"real world\"*, that this action should have a higher probability of getting itself more <span style=\"color:green\">rewards</span> in the future. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e59f11c-c642-4703-900f-9caa61a3d7cb",
   "metadata": {},
   "source": [
    "That's it. This is the step-by-step process that is at the heart of many reinforcement learning algorithms. The agent makes a <span style=\"color:blue\">prediction</span>, *compares* it to a **single sample of reality**. Then it nudges its internal beliefs to be slightly more accurate next time. \n",
    "\n",
    "We *know* from our ML series that it is called **stochastic gradient descent** because the nudge is based on a random sample of the real word - that single <span style=\"color:green\">'reward'</span>. It is not exact, so we don't need to take the expectation over all over samples. That's also, in some sense, how we deal with actually just doing something practical. \n",
    "\n",
    "Now that you understand the logic of a single update, you can just put it inside a `for-loop` and let *Scalebandit* learn from thousands or even millions of experiences. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67abcf8-d231-46fe-bdb6-c02bf360daf7",
   "metadata": {},
   "source": [
    "\n",
    "#### Bonus: A lookup table is a special form of linear function approximation\n",
    "\n",
    "Let's show this relationship. The idea is pretty huge, because it birdges to what we did last lecture. It means we didn't **throw away our old method**, we <span style=\"color:#088F8F\">generalized it</span>. Let's think about what we just built.\n",
    "\n",
    "1. Our *Personalbandit* from Lecture 2 used a 2D table `belief_table[context_idx, action_idx]` to look up the right user and then select the appropriate thumbnail to show.\n",
    "2. Our new *Scalebandit* uses a weight matrix `W` and a list which were 0 everywhere, exept for the user which had just shown up, that index was 1 (this is called 'one-hot encoded'). Then we had our beliefs be `s @ W`.\n",
    "\n",
    "Let's trace the prediction for a 'new viewer' (i.e., context_idx=0).\n",
    "\n",
    "- The state vector is [1, 0]\n",
    "- The prediction is [1, 0] @ [[W_00, W_01, W_02], [W_10, W_11, W_12]] -> (1, 2) x (2, 3) -> (1, 3)\n",
    "- The result of this dot product is simply `[W_00, W_01, W_02]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce8d8163-0f75-4a66-a3f0-5edc01bb6f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_viewer = [1, 0] # just a one representing the type of person that has arrived at our YouTube page\n",
    "subscriber = [0, 1] # the same for this type of user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1e1a01a2-20ec-486f-8c68-2b176ddd895c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's say a new user showed up, and we will trace this\n",
    "state = new_viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c6e981e3-0b5c-44c4-8726-e1abf2a936de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.65998573, -1.54196234, -0.05353478],\n",
       "       [-1.20342247,  1.10157142,  0.96988953]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed = 42\n",
    "W = np.random.randn(env.num_contexts, env.num_actions)\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d2b13ea9-8c4f-44a9-a71a-a2cd564c4dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.65998573, -1.54196234, -0.05353478])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see what row we have 'looked up' in our belief table? \n",
    "state @ W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f420996a-6152-4a4b-b127-7e525545e120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see that the dot-product is the same as indexing into our 'table' with index 0 (for 'new user' from Lecture 2)\n",
    "state @ W == W[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f64837ff-7dec-4dea-904c-ad27bdc21be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's say the other user shows up (i.e., context_idx=1), then state is \n",
    "state = subscriber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c159686e-c246-4bb9-8376-1ca555e454c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.79675108, -1.49999193, -1.13451925])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and what happens to the dot product now?\n",
    "state @ W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f95275c8-787f-4025-abcf-25b1a0de3906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# again, equivalent with indexing into our 2D table with index 1 (for 'subscriber' from Lecture 2)\n",
    "state @ W == W[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ba73ba-f84b-4d6e-8e51-013d24bc94cd",
   "metadata": {},
   "source": [
    "Wooot Our weight matrix, `W` is behaving *exactly like our 2D belief table* from Lecture 2.\n",
    "\n",
    "- Lookng up `belief_table[0]`, gave us the values for a 'new viewer'\n",
    "- Multiplying by `s=[1,0]` gives us the first row of our belief table, which are the values that correspond to what we saw in the last lecture\n",
    "\n",
    "They are mathematically identical!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f9db2b02-ce9a-44e9-bb30-7063814208c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.16676071, -0.05388295,  0.59151965],\n",
       "       [ 0.58147024, -0.70740083, -2.07200065]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and how did we build our 2D 'belief table' in Lecture 2?\n",
    "belief_table = np.random.randn(env.num_contexts, env.num_actions) # say our belief table from Lecture 2 had these numbers\n",
    "belief_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6feb65-e0e7-4d15-9523-1e054a53bf3a",
   "metadata": {},
   "source": [
    "What we have just discovered is a fundamental truth: **a table is a special case of a linear model**. Therefore, a lookup table is a special cas of linear value function approximation where the features are a list with a 1 for the viewer that shows up (or \"one-hot encoded\" indicator vectors).\n",
    "\n",
    "In our case, \"being a new viewer\" is a single feature, and \"being a subscriber\" is another. Because our features perfectly desribe our states and don't overlap, the linear model learns to dedicate one row of weights to *each feature*, recreating the lookup table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f621aa-a51d-4a67-a8ca-44e9cea1fdc5",
   "metadata": {},
   "source": [
    "So, why does this matter? If they are the same, why did we do all this work?\n",
    "\n",
    "The reason is the because this new perspective is *far more powerful* and unlocksk one solution method to the scaling problem. \n",
    "\n",
    "----\n",
    "\n",
    "Let's go back to the \"exploding tables\" problem:\n",
    "\n",
    "- Contexts: User type (2), device (3), country (20). Total states = 120.\n",
    "- Table approach: needs a giant table with 120 rows.\n",
    "\n",
    "---\n",
    "\n",
    "Now, let's think in therms of **features** for our linear model:\n",
    "\n",
    "- Instead of a single big list of size 120, we can represent our state with three separate lists, concatinated together:\n",
    "- <span style=\"color:#808000\">User type</span>: `[1, 0]` ('new viewer')\n",
    "- <span style=\"color:darkgray\">Device</span>: `[0, 1, 0]` (for desktop)\n",
    "- <span style=\"color:#CD7F32\">Country</span>: `[0, ..., 1, 0]` (for France)\n",
    "- Our <span style=\"color:blue\">state</span>`s` would be a long binary vector (a list with some numbers) of size 2 + 3 + 20 = 25.\n",
    "- Our <span style=\"color:purple\">weight matrix</span> `W` would have the shape [25, num_actions]. It would now have **25 rows**, <span style=\"color:red\">not</span> 120!\n",
    "\n",
    "The model would learn separate weights for the feature \"new user\", the feature \"is on a mobile device\" and the feature \"is in France\". When a new user shows up and is on a mobile device in France, the <span style=\"color:blue\">agent</span> would **add the contributions** of these learned weights together to amek a prediction.\n",
    "\n",
    "This is what we call <span style=\"color:#088F8F\">generalisation</span>. Our agent can learn that \"mobile users\" generally prefer short content, and apply that knowledge to a user from a country it has barely seen before. A lookup table could never do this. \n",
    "\n",
    "So, while our simple implementation was equivalent to a table, the **framework** we are building out, i.e., *function approximation* is what allows us to escape the curse of dimensionality. This is the key that will allow us to build real reinforcement learning agent that can go out into the world and solve large, interesting problems. This is also our first step towards what will eventually become **Deep Reinforcement Learning**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083fde58-3249-4916-b817-e0bef4be395f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8463071-5c29-40ef-b7b9-f9ce1ffb186f",
   "metadata": {},
   "source": [
    "\n",
    "#### Bonus 2: Why havent we seen the famous Q-learning algorithm yet?\n",
    "\n",
    "Some of you may have heard of a famous concept in reinforcement learning called Q-learning, temporal-difference learning. Is this what this is?\n",
    "\n",
    "The short answer is **no**. The error we are using here is called a **Monte Carlo** update. \n",
    "\n",
    "---\n",
    "Why don't we have a TD error?\n",
    "\n",
    "In our problem:\n",
    "1. we take an action\n",
    "2. the episode immediately terminates\n",
    "3. there is **no `S_t+1`**. There is no *next state*. The future doen't exist.\n",
    "\n",
    "If we *tried* to write the TD target for our problem, the `V(S_t+1)` term would be zero, `Q(S_t+1, a')`or becase the game is over. The formula would collapse:\n",
    "\n",
    "`TD Target = r + Î³ * 0` = `r`\n",
    "\n",
    "So, for one-step problems like ours, the **Monte Carlo** target and the **TD target** are identical. \n",
    "\n",
    "In summary: we have been using a Monte Carlo-style update because our problem is a one-step MDP. We haven't needed to worry about the future yet. \n",
    "\n",
    "However, to solve problems like  robotics, games, energy, or anything with a sequence of steps, we must learn to handle delayed <span style=\"color:green\">rewards</span> and estimate the value of <span style=\"color:#CCCCFF\t\">future states</span>. This will require  us to fully embrace the concept of temporal-difference learning, Bellman equations and that mysterious little gamma thing. This awaits us going forward!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ea09b3-4a3c-4822-a4ee-f2150ef9df02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
